{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cff6badd-013a-4a84-b26f-37531d52067b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sms_spam_collection\\SMSSpamCollection.tsv already exists, skipping download and unzipping.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import ssl\n",
    "import zipfile\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
    "    if data_file_path.exists():\n",
    "        print(f'{data_file_path} already exists, skipping download and unzipping.')\n",
    "        return\n",
    "\n",
    "    # Download the file WITHOUT custom SSL context\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        with open(zip_path, 'wb') as out_file:\n",
    "            out_file.write(response.read())\n",
    "\n",
    "    # Unzip the file\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "\n",
    "    # Rename the file to have .tsv extension\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)\n",
    "    print(f'File downloaded and saved as {data_file_path}')\n",
    "\n",
    "\n",
    "# Run the function\n",
    "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a1eac54-f822-4171-8d4c-61465cb81a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Update the path if necessary\n",
    "data_file_path = \"sms_spam_collection/SMSSpamCollection.tsv\"\n",
    "\n",
    "# Load it into a DataFrame\n",
    "df = pd.read_csv(data_file_path, sep='\\t', header=None, names=[\"label\", \"message\"])\n",
    "\n",
    "# Show first 5 rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0b4d148-d2f2-4b8e-9d1b-9ff0f60af39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#creating a value counts to see the classifications\n",
    "print(df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5a6221f-6ebc-4d52-be2a-78cd270a795e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "ham     747\n",
      "spam    747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#we then proceed to make the dataset balance\n",
    "# Load the dataset (adjust the path if needed)\n",
    "\n",
    "data_file_path = \"sms_spam_collection/SMSSpamCollection.tsv\"\n",
    "\n",
    "df = pd.read_csv(data_file_path, sep='\\t', header=None, names=[\"label\", \"message\"])\n",
    "\n",
    "def create_balanced_dataset(df):\n",
    "    # Count the number of spam messages\n",
    "    num_spam = df[df[\"label\"] == 'spam'].shape[0]\n",
    "\n",
    "    # Randomly sample ham messages to match the number of spam messages\n",
    "    ham_subset = df[df[\"label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "\n",
    "    # Combine the ham subset with all spam messages\n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"label\"] == \"spam\"]])\n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "# Now create the balanced dataset\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "\n",
    "# Check the class distribution\n",
    "print(balanced_df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18e25752-ba22-4d16-a2ac-bb4cf32ef0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we then convert the string class labels ie ham and spam into 1 and 0 respectively\n",
    "balanced_df['label'] = balanced_df['label'].map({'ham':0, 'spam':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78df46e0-7690-4cd2-8bd1-081c759fd40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a random split function to split the dataset into 3\n",
    "#the 3 are training data, validation data and test data\n",
    "#the ratio is usually 7.1.2\n",
    "#or 70% to train, 10% to validate and 20% to test\n",
    "def random_split(df, train_frac, validation_frac):\n",
    "    #shuffle the entire dataset first\n",
    "    df = df.sample(frac = 1, random_state = 123).reset_index(drop = True)\n",
    "\n",
    "    #calc split indices\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "\n",
    "    #split the dataframe\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end : validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8132ba94-c3be-4c95-986e-db84094950e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1045\n",
      "149\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df))\n",
    "print(len(validation_df))\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5209fb3-7aba-4c63-945c-8167ef0c0487",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataframes to csv file to reuse later\n",
    "train_df.to_csv('train.csv', index = None)\n",
    "validation_df.to_csv('validation.csv', index = None)\n",
    "test_df.to_csv('test.csv', index = None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7ce84eb-b156-4747-b9e9-a2258dd9587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        # Drop rows where 'label' or 'message' is NaN\n",
    "        self.data = self.data.dropna(subset=['label', 'message'])\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "        self.label2id = {'ham': 0, 'spam': 1}\n",
    "\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data['message']\n",
    "        ]\n",
    "\n",
    "        if max_length is None:\n",
    "            self.max_length = max(len(x) for x in self.encoded_texts)\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = int(self.data.iloc[index]['label'])  # Use directly if already 0/1\n",
    "        return (\n",
    "        torch.tensor(encoded, dtype=torch.long),\n",
    "        torch.tensor(label, dtype=torch.long)\n",
    "    )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6b5fa029-f014-48b5-84f5-62097d03f2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SpamDataset(csv_file='train.csv', tokenizer=tokenizer)\n",
    "\n",
    "print(train_dataset.data['label'].isna().sum())     # Check how many missing labels\n",
    "print(train_dataset.data['message'].isna().sum())   # Check how many missing messages\n",
    "print(train_dataset.data['label'].unique())         # See label values (e.g., 'ham', 'spam')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be973aa0-14ae-4f40-8fd4-67c54d40555a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "120\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # since GPT-2 has no official padding token\n",
    "\n",
    "\n",
    "#padding the train dataset\n",
    "train_dataset = SpamDataset(\n",
    "    csv_file = \"train.csv\",\n",
    "    max_length = None,\n",
    "    tokenizer = tokenizer\n",
    ")\n",
    "\n",
    "#padding the validation dataset\n",
    "validation_dataset = SpamDataset(\n",
    "    csv_file = \"validation.csv\",\n",
    "    max_length = train_dataset.max_length,\n",
    "    tokenizer = tokenizer\n",
    ")\n",
    "\n",
    "#padding the test dataset\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file = \"test.csv\",\n",
    "    max_length = train_dataset.max_length,\n",
    "    tokenizer = tokenizer\n",
    ")\n",
    "print(train_dataset.max_length)\n",
    "print(validation_dataset.max_length)\n",
    "print(test_dataset.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "baea2994-3a09-4c94-8717-5e7b02b291ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.bash_history', '.cache', '.conda', '.git', '.gitattributes', '.gitconfig', '.ipynb_checkpoints', '.ipython', '.jupyter', '.keras', '.ms-ad', '.viminfo', '.vscode', '4.66', 'App', 'AppData', 'Application Data', 'Contacts', 'Cookies', 'dataloaders.ipynb', 'Desktop', 'Documents', 'downloading_and_processing_dataset.ipynb', 'Downloads', 'Evaluating LLM Performance on Real Dataset.ipynb', 'Favorites', 'get_download.py', 'gpt2 weights saving and loading.ipynb', 'gpt2.ipynb.txt', 'gpt2model.pth', 'gpt2_dummy_weights.pth', 'gpt2_local_backup', 'gpt_download3.py', 'initializing_model_for_finetuning.ipynb', 'IntelGraphicsProfiles', 'Links', 'llm-1', 'loading_and_saving_openai_weights.ipynb', 'Local Settings', 'local_backup_loading_and_saving.ipynb', 'miniconda3', 'model_and_optimizer.pth', 'model_and_optimizer2.pth', 'Music', 'My Documents', 'NetHood', 'New folder', 'NTUSER.DAT', 'ntuser.dat.LOG1', 'ntuser.dat.LOG2', 'NTUSER.DAT{7376fb3e-2479-11f0-801d-80b6559a5fec}.TM.blf', 'NTUSER.DAT{7376fb3e-2479-11f0-801d-80b6559a5fec}.TMContainer00000000000000000001.regtrans-ms', 'NTUSER.DAT{7376fb3e-2479-11f0-801d-80b6559a5fec}.TMContainer00000000000000000002.regtrans-ms', 'ntuser.ini', 'OneDrive', 'Pictures', 'PrintHood', 'python', 'README.md', 'Recent', 'Saved Games', 'saving_loading_weights.ipynb', 'Searches', 'SendTo', 'sms_spam_collection', 'sms_spam_collection.zip', 'Start Menu', 'Templates', 'test.csv', 'the-verdict.txt', 'train.csv', 'Untitled.ipynb', 'Untitled1.ipynb', 'Untitled2.ipynb', 'Untitled3.ipynb', 'validation.csv', 'Videos', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c9cf17c-0d8e-441f-ae91-c811626cabb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['label', 'message'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "print(df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "baf3048c-2099-4716-95f4-aca415dbfd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating the dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = [item[0] for item in batch]\n",
    "    labels = torch.stack([item[1] for item in batch])\n",
    "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    return inputs_padded, labels\n",
    "    \n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "validation_loader = DataLoader(\n",
    "    dataset = validation_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset = test_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1592cafa-cd7e-4cd3-af85-9b1d295a8557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader:\n",
      "Input batch dimensions:  torch.Size([5, 50])\n",
      "Label batch dimensions:  torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "#to ensure that the dataloaders are working and are indeed returning batches of the expected size, we iterate over the training\n",
    "#loader and the print the tensor dimensions of the last batch\n",
    "print(\"Train Loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions: \", input_batch.shape)\n",
    "print(\"Label batch dimensions: \", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4cdfbc36-8aa7-4750-8238-c819c77fc145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "#print the no of batches in each dataset\n",
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(validation_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "93494988-b22c-496a-a6eb-55d3aba8e05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188\n"
     ]
    }
   ],
   "source": [
    "print(131+19+38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "32f67f63-7f86-4bc1-94c5-ab1469a8efef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing a model with pretained weights for finetuning\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASIC_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"max_seq_len\": 1024,\n",
    "    \"dropout\": 0.0,\n",
    "    \"embed_dim\": 768,\n",
    "    \"num_layers\": 12,\n",
    "    \"num_heads\": 12\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\" : {\"embed_dim\": 768, \"num_layers\": 12, \"num_heads\": 12},\n",
    "    \"gpt2-medium (355M)\" : {\"embed_dim\": 1024, \"num_layers\": 24, \"num_heads\": 16},\n",
    "    \"gpt2-large (774M)\" : {\"embed_dim\": 1280, \"num_layers\": 36, \"num_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\" : {\"embed_dim\": 1600, \"num_layers\": 48, \"num_heads\": 25},\n",
    "}\n",
    "\n",
    "BASIC_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"max_seq_len\": 1024,\n",
    "    \"dropout\": 0.0\n",
    "}\n",
    "\n",
    "BASIC_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "assert train_dataset.max_length <= BASIC_CONFIG[\"max_seq_len\"], (\n",
    "    f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n",
    "    f\"length {BASIC_CONFIG['max_seq_len']}. Reinitialize datasets with \"\n",
    "    f\"max_length={BASIC_CONFIG['max_seq_len']}\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2d1aaa74-7559-4c12-acaf-c0512ae20c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading and loading the model\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\" (\").rstrip(\")\")\n",
    "from gpt_download3 import download_and_load_gpt2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0c14a974-8639-472e-b0c2-a63102c35883",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "# ========== GPT2 Model Definition ==========\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.qkv_proj(x).view(B, T, 3, self.num_heads, self.head_dim)\n",
    "        q, k, v = qkv.unbind(dim=2)\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        attn_scores = attn_scores.masked_fill(torch.tril(torch.ones(T, T, device=x.device)) == 0, float('-inf'))\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        out = attn_weights @ v\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.o_proj(out)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.ffn = FeedForward(embed_dim, dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.dropout(self.attn(self.norm1(x)))\n",
    "        x = x + self.dropout(self.ffn(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, embed_dim, num_heads, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = nn.Embedding(max_seq_len, embed_dim)\n",
    "        self.trf_block = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim, num_heads, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.final_norm = nn.LayerNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.size()\n",
    "        tok_embed = self.token_emb(x)\n",
    "        pos_ids = torch.arange(T, device=x.device).unsqueeze(0)\n",
    "        pos_embed = self.pos_emb(pos_ids)\n",
    "        x = tok_embed + pos_embed\n",
    "        x = self.trf_block(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "78ef81c3-0612-4493-a077-77ef4cba73c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\miniconda3\\envs\\llm-env\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\miniconda3\\envs\\llm-env\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\encoder.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\miniconda3\\envs\\llm-env\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\hparams.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\miniconda3\\envs\\llm-env\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.data-00000-of-00001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\miniconda3\\envs\\llm-env\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\miniconda3\\envs\\llm-env\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.meta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\miniconda3\\envs\\llm-env\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "#loading the pretrained weights\n",
    "settings, params =  download_and_load_gpt2(model_size = model_size, models_dir = \"gpt2\")\n",
    "model = GPTModel(**BASIC_CONFIG)\n",
    "def load_weights_into_gpt(model, params):\n",
    "    model_dict = model.state_dict()\n",
    "    for name, param in params.items():\n",
    "        if name in model_dict:\n",
    "            try:\n",
    "                model_dict[name].copy_(param)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {name}: {e}\")\n",
    "\n",
    "\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c502195f-f646-4da2-8c88-331f8cf59923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "every effort moves you [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0]\n"
     ]
    }
   ],
   "source": [
    "#testing whether the model was loaded successfully\n",
    "import torch\n",
    "\n",
    "# Input text\n",
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "# Tokenizer utilities\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    return torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    return tokenizer.decode(token_ids.tolist(), skip_special_tokens=True)\n",
    "\n",
    "# Text generation\n",
    "@torch.no_grad()\n",
    "@torch.no_grad()\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    model.eval()\n",
    "    idx = idx.clone().detach().unsqueeze(0)  # (1, sequence_length)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        outputs = model(idx_cond)\n",
    "        logits = outputs.logits  # likely shape (1, vocab_size)\n",
    "\n",
    "        # If logits is 2D, skip indexing; otherwise take last token\n",
    "        if logits.dim() == 3:\n",
    "            logits = logits[:, -1, :]\n",
    "        \n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, next_token), dim=1)\n",
    "\n",
    "    return idx[0]\n",
    "\n",
    "# Generate text\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASIC_CONFIG[\"max_seq_len\"]\n",
    ")\n",
    "\n",
    "# Decode and print generated text\n",
    "print(token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "36e14955-e57a-4da8-8dc7-37fd4e7a8b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from huggingface_hub import login\n",
    "\n",
    "#login(token=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "54c259d8-744f-4790-8fff-d04ccbac3523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'user', 'id': '683cb0c3a69d5f276bbb4d97', 'name': 'Okoth67', 'fullname': 'Brian Okoth', 'email': 'bbollo386@gmail.com', 'emailVerified': True, 'canPay': False, 'periodEnd': None, 'isPro': False, 'avatarUrl': '/avatars/4af57a6209ab5488520bc60485f444df.svg', 'orgs': [], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'llm-1', 'role': 'read', 'createdAt': '2025-06-07T09:51:00.695Z'}}}\n"
     ]
    }
   ],
   "source": [
    "#from huggingface_hub import whoami\n",
    "\n",
    "#user_info = whoami()\n",
    "#print(user_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "76943c93-cea0-4d70-82c6-d5efb4516db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "#from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "#hg_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "#print(\"Model and tokenizer loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e6f1d320-85a1-42f6-91b3-aa4585f0f9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1778ec42-bf7f-49fb-a22f-5ad5f042738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#freezing the model before classification finetuning\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9575967c-91c0-483c-837a-8c0afd7438ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#then we replace the output layer (model.out_head) \n",
    "torch.manual_seed(123)\n",
    "num_classes = 2\n",
    "model.out_head =  torch.nn.Linear(in_features = BASIC_CONFIG[\"embed_dim\"], out_features = num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5cdf87bb-a06a-45fa-9f41-eabd304a209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lastly we configure the last transformer block and LayerNorm module\n",
    "# Unfreeze the last transformer block\n",
    "for param in model.distilbert.transformer.layer[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Unfreeze the final LayerNorm (called output_layer_norm in the last transformer block)\n",
    "for param in model.distilbert.transformer.layer[-1].output_layer_norm.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9de42ec7-08fd-4fbc-b9fb-d6051844b530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs : tensor([[ 101, 2079, 2017, 2031, 2051,  102]])\n",
      "Inputs dimensions:  torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"Do you have time\")\n",
    "inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "print(\"Inputs :\", inputs)\n",
    "print(\"Inputs dimensions: \", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "42b6853a-08ee-4f55-8c5f-a04c6ccd4da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs:  SequenceClassifierOutput(loss=None, logits=tensor([[ 1.7597, -1.4899]]), hidden_states=None, attentions=None)\n",
      "Output dimensions:  torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "#then we can pass encoded token IDs to the model as usual\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "print(\"Outputs: \", outputs)\n",
    "print(\"Output dimensions: \", outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e37a5ab5-8bd7-4b16-9384-2607e9bfcf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ 1.7597, -1.4899]])\n",
      "Shape: torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "#to extract the last output token from the output tensor\n",
    "print(\"Logits:\", outputs.logits)\n",
    "print(\"Shape:\", outputs.logits.shape)  # Should be (batch_size, num_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ec7bac-2eb6-4919-911c-5e181ffd3829",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
