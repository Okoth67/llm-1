{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cff6badd-013a-4a84-b26f-37531d52067b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sms_spam_collection\\SMSSpamCollection.tsv already exists, skipping download and unzipping.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import ssl\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
    "    if data_file_path.exists():\n",
    "        print(f'{data_file_path} already exists, skipping download and unzipping.')\n",
    "        return\n",
    "\n",
    "    # Download the file WITHOUT custom SSL context\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        with open(zip_path, 'wb') as out_file:\n",
    "            out_file.write(response.read())\n",
    "\n",
    "    # Unzip the file\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "\n",
    "    # Rename the file to have .tsv extension\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)\n",
    "    print(f'File downloaded and saved as {data_file_path}')\n",
    "\n",
    "\n",
    "# Run the function\n",
    "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8a1eac54-f822-4171-8d4c-61465cb81a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Update the path if necessary\n",
    "data_file_path = \"sms_spam_collection/SMSSpamCollection.tsv\"\n",
    "\n",
    "# Load it into a DataFrame\n",
    "df = pd.read_csv(data_file_path, sep='\\t', header=None, names=[\"label\", \"message\"])\n",
    "\n",
    "# Show first 5 rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d0b4d148-d2f2-4b8e-9d1b-9ff0f60af39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#creating a value counts to see the classifications\n",
    "print(df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f5a6221f-6ebc-4d52-be2a-78cd270a795e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "ham     747\n",
      "spam    747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#we then proceed to make the dataset balance\n",
    "# Load the dataset (adjust the path if needed)\n",
    "\n",
    "data_file_path = \"sms_spam_collection/SMSSpamCollection.tsv\"\n",
    "\n",
    "df = pd.read_csv(data_file_path, sep='\\t', header=None, names=[\"label\", \"message\"])\n",
    "\n",
    "def create_balanced_dataset(df):\n",
    "    # Count the number of spam messages\n",
    "    num_spam = df[df[\"label\"] == 'spam'].shape[0]\n",
    "\n",
    "    # Randomly sample ham messages to match the number of spam messages\n",
    "    ham_subset = df[df[\"label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "\n",
    "    # Combine the ham subset with all spam messages\n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"label\"] == \"spam\"]])\n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "# Now create the balanced dataset\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "\n",
    "# Check the class distribution\n",
    "print(balanced_df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "18e25752-ba22-4d16-a2ac-bb4cf32ef0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we then convert the string class labels ie ham and spam into 1 and 0 respectively\n",
    "balanced_df['label'] = balanced_df['label'].map({'ham':0, 'spam':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "78df46e0-7690-4cd2-8bd1-081c759fd40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a random split function to split the dataset into 3\n",
    "#the 3 are training data, validation data and test data\n",
    "#the ratio is usually 7.1.2\n",
    "#or 70% to train, 10% to validate and 20% to test\n",
    "def random_split(df, train_frac, validation_frac):\n",
    "    #shuffle the entire dataset first\n",
    "    df = df.sample(frac = 1, random_state = 123).reset_index(drop = True)\n",
    "\n",
    "    #calc split indices\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "\n",
    "    #split the dataframe\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end : validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8132ba94-c3be-4c95-986e-db84094950e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1045\n",
      "149\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df))\n",
    "print(len(validation_df))\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b5209fb3-7aba-4c63-945c-8167ef0c0487",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataframes to csv file to reuse later\n",
    "train_df.to_csv('train.csv', index = None)\n",
    "validation_df.to_csv('validation.csv', index = None)\n",
    "test_df.to_csv('test.csv', index = None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e7ce84eb-b156-4747-b9e9-a2258dd9587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        # Drop rows where 'label' or 'message' is NaN\n",
    "        self.data = self.data.dropna(subset=['label', 'message'])\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "        self.label2id = {'ham': 0, 'spam': 1}\n",
    "\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data['message']\n",
    "        ]\n",
    "\n",
    "        if max_length is None:\n",
    "            self.max_length = max(len(x) for x in self.encoded_texts)\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = int(self.data.iloc[index]['label'])  # Use directly if already 0/1\n",
    "        return (\n",
    "        torch.tensor(encoded, dtype=torch.long),\n",
    "        torch.tensor(label, dtype=torch.long)\n",
    "    )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6b5fa029-f014-48b5-84f5-62097d03f2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SpamDataset(csv_file='train.csv', tokenizer=tokenizer)\n",
    "\n",
    "print(train_dataset.data['label'].isna().sum())     # Check how many missing labels\n",
    "print(train_dataset.data['message'].isna().sum())   # Check how many missing messages\n",
    "print(train_dataset.data['label'].unique())         # See label values (e.g., 'ham', 'spam')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "be973aa0-14ae-4f40-8fd4-67c54d40555a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "120\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#padding the train dataset\n",
    "train_dataset = SpamDataset(\n",
    "    csv_file = \"train.csv\",\n",
    "    max_length = None,\n",
    "    tokenizer = tokenizer\n",
    ")\n",
    "\n",
    "#padding the validation dataset\n",
    "validation_dataset = SpamDataset(\n",
    "    csv_file = \"validation.csv\",\n",
    "    max_length = train_dataset.max_length,\n",
    "    tokenizer = tokenizer\n",
    ")\n",
    "\n",
    "#padding the test dataset\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file = \"test.csv\",\n",
    "    max_length = train_dataset.max_length,\n",
    "    tokenizer = tokenizer\n",
    ")\n",
    "print(train_dataset.max_length)\n",
    "print(validation_dataset.max_length)\n",
    "print(test_dataset.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baea2994-3a09-4c94-8717-5e7b02b291ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.bash_history', '.cache', '.conda', '.git', '.gitattributes', '.gitconfig', '.ipynb_checkpoints', '.ipython', '.jupyter', '.keras', '.ms-ad', '.viminfo', '.vscode', '4.66', 'App', 'AppData', 'Application Data', 'Contacts', 'Cookies', 'Desktop', 'Documents', 'downloading_and_processing_dataset.ipynb', 'Downloads', 'Evaluating LLM Performance on Real Dataset.ipynb', 'Favorites', 'get_download.py', 'gpt2', 'gpt2 weights saving and loading.ipynb', 'gpt2.ipynb.txt', 'gpt2model.pth', 'gpt2_dummy_weights.pth', 'gpt_download3.py', 'IntelGraphicsProfiles', 'Links', 'llm-1', 'loading_and_saving_openai_weights.ipynb', 'Local Settings', 'local_backup_loading_and_saving.ipynb', 'miniconda3', 'model_and_optimizer.pth', 'model_and_optimizer2.pth', 'Music', 'My Documents', 'NetHood', 'New folder', 'NTUSER.DAT', 'ntuser.dat.LOG1', 'ntuser.dat.LOG2', 'NTUSER.DAT{7376fb3e-2479-11f0-801d-80b6559a5fec}.TM.blf', 'NTUSER.DAT{7376fb3e-2479-11f0-801d-80b6559a5fec}.TMContainer00000000000000000001.regtrans-ms', 'NTUSER.DAT{7376fb3e-2479-11f0-801d-80b6559a5fec}.TMContainer00000000000000000002.regtrans-ms', 'ntuser.ini', 'OneDrive', 'Pictures', 'PrintHood', 'python', 'README.md', 'Recent', 'Saved Games', 'saving_loading_weights.ipynb', 'Searches', 'SendTo', 'sms_spam_collection', 'sms_spam_collection.zip', 'Start Menu', 'Templates', 'test.csv', 'the-verdict.txt', 'train.csv', 'Untitled.ipynb', 'Untitled1.ipynb', 'Untitled2.ipynb', 'Untitled3.ipynb', 'validation.csv', 'Videos', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8c9cf17c-0d8e-441f-ae91-c811626cabb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['label', 'message'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "print(df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "baf3048c-2099-4716-95f4-aca415dbfd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating the dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = [item[0] for item in batch]\n",
    "    labels = torch.stack([item[1] for item in batch])\n",
    "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    return inputs_padded, labels\n",
    "    \n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "validation_loader = DataLoader(\n",
    "    dataset = validation_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset = test_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1592cafa-cd7e-4cd3-af85-9b1d295a8557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader:\n",
      "Input batch dimensions:  torch.Size([5, 50])\n",
      "Label batch dimensions:  torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "#to ensure that the dataloaders are working and are indeed returning batches of the expected size, we iterate over the training\n",
    "#loader and the print the tensor dimensions of the last batch\n",
    "print(\"Train Loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions: \", input_batch.shape)\n",
    "print(\"Label batch dimensions: \", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4cdfbc36-8aa7-4750-8238-c819c77fc145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "#print the no of batches in each dataset\n",
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(validation_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "93494988-b22c-496a-a6eb-55d3aba8e05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188\n"
     ]
    }
   ],
   "source": [
    "print(131+19+38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f67f63-7f86-4bc1-94c5-ab1469a8efef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
